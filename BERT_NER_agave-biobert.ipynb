{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_JyRck8UnFjj"
   },
   "outputs": [],
   "source": [
    "#!wget -O scibert_uncased.tar https://github.com/naver/biobert-pretrained/releases/download/v1.1-pubmed/biobert_v1.1_pubmed.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tar -xvf scibert_uncased.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import torch\\nfrom transformers import BertTokenizer, BertModel\\nimport argparse\\nimport logging\\n\\nfrom transformers import BertConfig, BertForPreTraining, load_tf_weights_in_bert\\n\\ndef convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, bert_config_file, pytorch_dump_path):\\n    # Initialise PyTorch model\\n    config = BertConfig.from_json_file(bert_config_file)\\n    print(\"Building PyTorch model from configuration: {}\".format(str(config)))\\n    model = BertForPreTraining(config)\\n\\n    # Load weights from tf checkpoint\\n    load_tf_weights_in_bert(model, config, tf_checkpoint_path)\\n\\n    # Save pytorch-model\\n    print(\"Save PyTorch model to {}\".format(pytorch_dump_path))\\n    torch.save(model.state_dict(), pytorch_dump_path)\\n\\nconvert_tf_checkpoint_to_pytorch(\"biobert_v1.1_pubmed/model.ckpt-1000000\", \"biobert_v1.1_pubmed/bert_config.json\", \"biobert_v1.1_pubmed/pytorch_model.bin\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "from transformers import BertConfig, BertForPreTraining, load_tf_weights_in_bert\n",
    "\n",
    "def convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, bert_config_file, pytorch_dump_path):\n",
    "    # Initialise PyTorch model\n",
    "    config = BertConfig.from_json_file(bert_config_file)\n",
    "    print(\"Building PyTorch model from configuration: {}\".format(str(config)))\n",
    "    model = BertForPreTraining(config)\n",
    "\n",
    "    # Load weights from tf checkpoint\n",
    "    load_tf_weights_in_bert(model, config, tf_checkpoint_path)\n",
    "\n",
    "    # Save pytorch-model\n",
    "    print(\"Save PyTorch model to {}\".format(pytorch_dump_path))\n",
    "    torch.save(model.state_dict(), pytorch_dump_path)\n",
    "\n",
    "convert_tf_checkpoint_to_pytorch(\"biobert_v1.1_pubmed/model.ckpt-1000000\", \"biobert_v1.1_pubmed/bert_config.json\", \"biobert_v1.1_pubmed/pytorch_model.bin\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!ls biobert_v1.1_pubmed\\n!mv biobert_v1.1_pubmed/bert_config.json biobert_v1.1_pubmed/config.json\\n!ls biobert_v1.1_pubmed'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!ls biobert_v1.1_pubmed\n",
    "!mv biobert_v1.1_pubmed/bert_config.json biobert_v1.1_pubmed/config.json\n",
    "!ls biobert_v1.1_pubmed\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0505 14:51:39.422375 47349498433216 file_utils.py:41] PyTorch version 1.2.0 available.\n",
      "I0505 14:51:43.623143 47349498433216 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertModel,BertTokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2733,
     "status": "ok",
     "timestamp": 1585290652633,
     "user": {
      "displayName": "Jae Choi",
      "photoUrl": "",
      "userId": "12753612472423284421"
     },
     "user_tz": 420
    },
    "id": "CoaXi2Xem14t",
    "outputId": "1ec1615f-9503-4889-98ca-b8f2fc3fbbd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May  5 14:51:45 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:18:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    59W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QsugR204m14y"
   },
   "outputs": [],
   "source": [
    "from create_processed_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1104,
     "status": "ok",
     "timestamp": 1585290675600,
     "user": {
      "displayName": "Jae Choi",
      "photoUrl": "",
      "userId": "12753612472423284421"
     },
     "user_tz": 420
    },
    "id": "l3OInENUm15K",
    "outputId": "f5b66e0e-3a06-4350-e281-6ac653f4a53c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QyNtpN9Tm15O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported all libraries\n",
      "Loaded device: cuda\n"
     ]
    }
   ],
   "source": [
    "print('Imported all libraries')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print('Loaded device:',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1014,
     "status": "ok",
     "timestamp": 1585290678529,
     "user": {
      "displayName": "Jae Choi",
      "photoUrl": "",
      "userId": "12753612472423284421"
     },
     "user_tz": 420
    },
    "id": "aovUCJbvm15R",
    "outputId": "10e75e8d-733e-43ee-f089-86b186679768"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 996,
     "status": "ok",
     "timestamp": 1585290680272,
     "user": {
      "displayName": "Jae Choi",
      "photoUrl": "",
      "userId": "12753612472423284421"
     },
     "user_tz": 420
    },
    "id": "UuwY9omJm15Y",
    "outputId": "3f275fde-a82c-4a42-938d-bbf0728206e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-SXM2-32GB'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = 53914\n",
    "MAX_LEN = 500\n",
    "SEED = 520\n",
    "bs = 1\n",
    "MAX_N_SPLITS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0505 14:51:47.145000 47349498433216 tokenization_utils.py:420] Model name './biobert_v1.1_pubmed/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming './biobert_v1.1_pubmed/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0505 14:51:47.147556 47349498433216 tokenization_utils.py:449] Didn't find file ./biobert_v1.1_pubmed/added_tokens.json. We won't load it.\n",
      "I0505 14:51:47.148517 47349498433216 tokenization_utils.py:449] Didn't find file ./biobert_v1.1_pubmed/special_tokens_map.json. We won't load it.\n",
      "I0505 14:51:47.149339 47349498433216 tokenization_utils.py:449] Didn't find file ./biobert_v1.1_pubmed/tokenizer_config.json. We won't load it.\n",
      "I0505 14:51:47.150114 47349498433216 tokenization_utils.py:502] loading file ./biobert_v1.1_pubmed/vocab.txt\n",
      "I0505 14:51:47.151080 47349498433216 tokenization_utils.py:502] loading file None\n",
      "I0505 14:51:47.151795 47349498433216 tokenization_utils.py:502] loading file None\n",
      "I0505 14:51:47.152448 47349498433216 tokenization_utils.py:502] loading file None\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./biobert_v1.1_pubmed/\"\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir, do_lower_case=True)  # Add specific options if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data\n"
     ]
    }
   ],
   "source": [
    "print('Loading training data')\n",
    "train_data_processor = DataProcessor('train',size=None,tokenizer=tokenizer)\n",
    "tags_vals = train_data_processor.get_tags_vals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure no paragraph has only 'O' tags\n",
    "paragraphs_tags =  train_data_processor.paragraph_tags\n",
    "inp = None\n",
    "for i, paragraph_tags in enumerate(paragraphs_tags):\n",
    "    assert not all(p == 'O' for p in paragraph_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tr_inputs,tr_masks,tr_tags,tr_ttids,train_tokenizer = train_data_processor.get_processed_data()\n",
    "#print('Loaded training data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_processor.maxN = 32, 32 * 424 = 13568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dev data\n"
     ]
    }
   ],
   "source": [
    "print('Loading dev data')\n",
    "valid_data_processor = DataProcessor('dev',size=None,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_inputs,val_masks,val_tags,val_ttids,train_val_tokenizer = valid_data_processor.get_processed_data()\n",
    "#print('Loaded dev data')\n",
    "#valid_data_processor.maxN = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "586TVHQfm17F"
   },
   "outputs": [],
   "source": [
    "#tr_inputs = torch.tensor(tr_inputs)\n",
    "#tr_tags = torch.tensor(tr_tags)\n",
    "#tr_masks = torch.tensor(tr_masks)\n",
    "#tr_ttids = torch.tensor(tr_ttids)\n",
    "#val_inputs = torch.tensor(val_inputs)\n",
    "#val_tags = torch.tensor(val_tags)\n",
    "#val_masks = torch.tensor(val_masks)\n",
    "#val_ttids = torch.tensor(val_ttids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NikvdHGBm17Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIPuS7ERm17S"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AeuiX7QPm17W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train and dev data loaders\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_data_processor, shuffle=True, batch_size=bs, collate_fn=train_data_processor.my_collate)\n",
    "\n",
    "valid_dataloader = DataLoader(valid_data_processor, shuffle=True, batch_size=bs, collate_fn=train_data_processor.my_collate)\n",
    "print('Created train and dev data loaders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2768,
     "status": "ok",
     "timestamp": 1585290822435,
     "user": {
      "displayName": "Jae Choi",
      "photoUrl": "",
      "userId": "12753612472423284421"
     },
     "user_tz": 420
    },
    "id": "ip0X1tT_m17g",
    "outputId": "9d45acc3-5e0a-4f50-850f-312968c1b4e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May  5 14:54:38 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:18:00.0 Off |                    0 |\n",
      "| N/A   44C    P0    45W / 300W |     11MiB / 32510MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24732,
     "status": "ok",
     "timestamp": 1585290847363,
     "user": {
      "displayName": "Jae Choi",
      "photoUrl": "",
      "userId": "12753612472423284421"
     },
     "user_tz": 420
    },
    "id": "uvUKzFJCm17n",
    "outputId": "20156062-ce30-4f4d-b5bb-9a4d5c480632"
   },
   "outputs": [],
   "source": [
    "#BertEmbed = BertModel.from_pretrained(\"bert-base-uncased\", num_labels=len(tags_vals))\n",
    "\n",
    "\n",
    "#output_dir = \"./models/\"\n",
    "# Step 2: Re-load the saved model and vocabulary\n",
    "\n",
    "# Example for a Bert model\n",
    "#model = BertForTokenClassification.from_pretrained(output_dir,num_labels=len(tags_vals))\n",
    "\n",
    "\n",
    "#print('Loaded BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0505 14:54:38.983725 47349498433216 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/akobtan/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I0505 14:54:38.986228 47349498433216 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0505 14:54:39.290673 47349498433216 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/akobtan/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 768\n",
    "KERNEL_SIZE = 21\n",
    "PADDING = (KERNEL_SIZE - 1) // 2\n",
    "import torch.nn as nn\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\",num_labels=len(tags_vals))\n",
    "class myNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l = nn.Conv1d(HIDDEN_SIZE,len(tags_vals),kernel_size = KERNEL_SIZE, padding = PADDING)\n",
    "        #perform softmax on classes dimension\n",
    "        #self.s = nn.Softmax(dim=1)\n",
    "    def forward(self,last_hidden_state):\n",
    "        last_hidden_state = last_hidden_state.transpose(1,2)\n",
    "        logits = self.l(last_hidden_state)\n",
    "        #probs = self.s(logits)\n",
    "        return logits\n",
    "mynet = myNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehN836-Qm17u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed BERT and Convolution to GPU\n"
     ]
    }
   ],
   "source": [
    "model.cuda();\n",
    "mynet.cuda();\n",
    "print('Pushed BERT and Convolution to GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kKtdGN6Cm17x",
    "outputId": "151204cb-68bb-4ed6-f5d7-ccd7f267e47b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May  5 14:54:44 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:18:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    59W / 300W |   1472MiB / 32510MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0    240102      C   /packages/7x/anaconda3/5.3.0/bin/python     1461MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3u9x1RqFm172"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nu4SG_6ym18G"
   },
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters()) + list(mynet.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TllKViELm18I"
   },
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aG3KTg9Am18P",
    "outputId": "b3e59b71-830f-4265-8fd8-abb711858c22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating training\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "print('Initiating training')\n",
    "closs = CrossEntropyLoss(ignore_index=-100)\n",
    "epochs = 4\n",
    "max_grad_norm = 1.0\n",
    "F_scores = np.zeros(epochs,float)\n",
    "patience = 6\n",
    "accumulation_steps = 16\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = (len(train_dataloader) / accumulation_steps) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "for e in trange(epochs):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        for split in range(len(batch[0])):\n",
    "            b_input_ids = batch[0].pop(0).to(device)\n",
    "            b_input_mask = batch[1].pop(0).to(device)\n",
    "            b_labels = batch[2].pop(0).to(device)\n",
    "            b_ttids = batch[3].pop(0).to(device)\n",
    "            # forward pass\n",
    "            # loss = model(b_input_ids, token_type_ids=b_ttids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            if split == 0:\n",
    "                last_hidden_state = model(b_input_ids, token_type_ids=b_ttids, attention_mask=b_input_mask)[0]\n",
    "            else:\n",
    "                last_hidden_state = torch.cat([last_hidden_state,model(b_input_ids, token_type_ids=b_ttids, attention_mask=b_input_mask)[0]], dim = 1)\n",
    "                b_labels = torch.cat([prev_labels,b_labels],dim=1)\n",
    "                b_input_ids = torch.cat([prev_ids, b_input_ids], dim = 1)\n",
    "            del b_input_mask, b_ttids\n",
    "            torch.cuda.empty_cache()\n",
    "            prev_labels = copy.copy(b_labels)\n",
    "            prev_ids = b_input_ids\n",
    "        del prev_labels, prev_ids\n",
    "        torch.cuda.empty_cache()\n",
    "        b_logits = mynet(last_hidden_state)\n",
    "        del last_hidden_state\n",
    "        b_logits = b_logits.transpose(1,2)\n",
    "        logits = b_logits.view(-1,len(tags_vals))\n",
    "        labels = b_labels.view(-1)\n",
    "        if len(logits) != len(labels):\n",
    "            print(b_input_ids)\n",
    "            print(train_data_processor.tokenizer.convert_ids_to_tokens(b_input_ids[0]))\n",
    "            print(b_input_ids.size())\n",
    "            print('Logits:')\n",
    "            print(logits)\n",
    "            print(logits.size())\n",
    "            print('Labels:')\n",
    "            print(labels)\n",
    "            print(labels.size())\n",
    "            exit()\n",
    "        loss = closs(logits, labels)\n",
    "        # backward pass\n",
    "        del logits, labels\n",
    "        torch.cuda.empty_cache()\n",
    "        b_logits = b_logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(b_logits, axis=2)])\n",
    "        true_labels.append(label_ids)\n",
    "        del b_logits, label_ids, b_labels\n",
    "        torch.cuda.empty_cache()\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += 1\n",
    "        nb_tr_steps += 1\n",
    "        if (step+1)%accumulation_steps == 0:\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_vals[l_ii] if l_ii!=-100 else '[PAD]' for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    c_f1 = f1_score(valid_tags, pred_tags)\n",
    "    rep = classification_report(valid_tags, pred_tags)\n",
    "    print('Training classification report:',rep)\n",
    "    print(\"Training F1-Score: {}\".format(c_f1))\n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        with torch.no_grad():\n",
    "            # add batch to gpu\n",
    "            for split in range(len(batch[0])):\n",
    "                b_input_ids = batch[0].pop(0).to(device)\n",
    "                b_input_mask = batch[1].pop(0).to(device)\n",
    "                b_labels = batch[2].pop(0).to(device)\n",
    "                b_ttids = batch[3].pop(0).to(device)\n",
    "               # forward pass\n",
    "                # loss = model(b_input_ids, token_type_ids=b_ttids, attention_mask=b_input_mask, labels=b_labels)\n",
    "                if split == 0:\n",
    "                    last_hidden_state = model(b_input_ids, token_type_ids=b_ttids, attention_mask=b_input_mask)[0]\n",
    "                else:\n",
    "                    last_hidden_state = torch.cat([last_hidden_state,model(b_input_ids, token_type_ids=b_ttids, attention_mask=b_input_mask)[0]], dim = 1)\n",
    "                    b_labels = torch.cat([prev_labels,b_labels],dim=1)\n",
    "                del b_input_ids, b_input_mask, b_ttids\n",
    "                torch.cuda.empty_cache()\n",
    "                prev_labels = copy.copy(b_labels)\n",
    "            del prev_labels\n",
    "            torch.cuda.empty_cache()\n",
    "            b_logits = mynet(last_hidden_state)\n",
    "            b_logits = b_logits.transpose(1,2)\n",
    "            logits = b_logits.view(-1,len(tags_vals))\n",
    "            labels = b_labels.view(-1)\n",
    "            tmp_eval_loss = closs(logits, labels)\n",
    "            del logits, labels\n",
    "            torch.cuda.empty_cache()\n",
    "        b_logits = b_logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(b_logits, axis=2)])\n",
    "        true_labels.append(label_ids)\n",
    "        tmp_eval_accuracy = flat_accuracy(b_logits, label_ids)\n",
    "        del b_logits, label_ids, b_labels\n",
    "        torch.cuda.empty_cache()\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_examples += 1\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_vals[l_ii] if l_ii!=-100 else '[PAD]' for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    c_f1 = f1_score(valid_tags, pred_tags)\n",
    "    rep = classification_report(valid_tags, pred_tags)\n",
    "    print(rep)\n",
    "    print(\"F1-Score: {}\".format(c_f1))\n",
    "    #k epochs no improvement\n",
    "    F_scores[e] = c_f1\n",
    "    prev_Fs = np.arange(e-patience,e)\n",
    "    prev_indices = prev_Fs[prev_Fs>=0]\n",
    "    if all(F_scores[e] - F_scores[prev_indices] < 0 ) and e > patience:\n",
    "        print('Done training')\n",
    "        break\n",
    "    else:\n",
    "        print('Still training')\n",
    "        #save checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uSZSRGJDm18Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating validation on dev set\n",
      "Validation loss: 0.05402455055537064\n",
      "Validation Accuracy: 0.9741180001942487\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      ans       0.40      0.29      0.34     63389\n",
      "    [PAD]       0.00      0.00      0.00     67403\n",
      "\n",
      "micro avg       0.40      0.14      0.21    130792\n",
      "macro avg       0.19      0.14      0.16    130792\n",
      "\n",
      "F1-Score: 0.20836896381830433\n"
     ]
    }
   ],
   "source": [
    "print('Initiating validation on dev set')\n",
    "# VALIDATION on validation set\n",
    "model.eval()\n",
    "mynet.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "predictions , true_labels = [], []\n",
    "for batch in valid_dataloader:\n",
    "    with torch.no_grad():\n",
    "        # add batch to gpu\n",
    "        for split in range(len(batch[0])):\n",
    "            b_input_ids = batch[0].pop(0).to(device)\n",
    "            b_input_mask = batch[1].pop(0).to(device)\n",
    "            b_labels = batch[2].pop(0).to(device)\n",
    "            b_ttids = batch[3].pop(0).to(device)\n",
    "           # forward pass\n",
    "            # loss = model(b_input_ids, token_type_ids=b_ttids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            if split == 0:\n",
    "                last_hidden_state = model(b_input_ids, token_type_ids=b_ttids, attention_mask=b_input_mask)[0]\n",
    "            else:\n",
    "                last_hidden_state = torch.cat([last_hidden_state,model(b_input_ids, token_type_ids=b_ttids, attention_mask=b_input_mask)[0]], dim = 1)\n",
    "                b_labels = torch.cat([prev_labels,b_labels],dim=1)\n",
    "            del b_input_ids, b_input_mask, b_ttids\n",
    "            torch.cuda.empty_cache()\n",
    "            prev_labels = copy.copy(b_labels)\n",
    "        del prev_labels\n",
    "        torch.cuda.empty_cache()\n",
    "        b_logits = mynet(last_hidden_state)\n",
    "        b_logits = b_logits.transpose(1,2)\n",
    "        logits = b_logits.view(-1,len(tags_vals))\n",
    "        labels = b_labels.view(-1)\n",
    "        tmp_eval_loss = closs(logits, labels)\n",
    "        del logits, labels\n",
    "        torch.cuda.empty_cache()\n",
    "    b_logits = b_logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    predictions.extend([list(p) for p in np.argmax(b_logits, axis=2)])\n",
    "    true_labels.append(label_ids)\n",
    "    tmp_eval_accuracy = flat_accuracy(b_logits, label_ids)\n",
    "    del b_logits, label_ids, b_labels\n",
    "    torch.cuda.empty_cache()\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_examples += 1\n",
    "    nb_eval_steps += 1\n",
    "eval_loss = eval_loss/nb_eval_steps\n",
    "print(\"Validation loss: {}\".format(eval_loss))\n",
    "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "valid_tags = [tags_vals[l_ii] if l_ii!=-100 else '[PAD]' for l in true_labels for l_i in l for l_ii in l_i]\n",
    "c_f1 = f1_score(valid_tags, pred_tags)\n",
    "rep = classification_report(valid_tags, pred_tags)\n",
    "print(rep)\n",
    "print(\"F1-Score: {}\".format(c_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8SxOsodim18g"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0507 10:38:58.924122 47349498433216 modeling.py:230] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./newmodel_notsure/vocab.txt',)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Saving model')\n",
    "from pytorch_pretrained_bert import WEIGHTS_NAME, CONFIG_NAME\n",
    "\n",
    "output_dir = \"./newmodel_notsure/\"\n",
    "\n",
    "# Step 1: Save a model, configuration and vocabulary that you have fine-tuned\n",
    "\n",
    "# If we have a distributed model, save only the encapsulated model\n",
    "# (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "\n",
    "# If we save using the predefined names, we can load using `from_pretrained`\n",
    "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "tokenizer.save_vocabulary(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data\n"
     ]
    }
   ],
   "source": [
    "print('Loading test data')\n",
    "test_data_processor = DataProcessor('test',size=None,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"more than meets the eye : the ‘ pink salmon patch ’ summary  ocular adnexal lymphomas account for 1 – 2 % of all non-hodgkin ' s lymphomas . conjunctiva is the primary site of involvement in one - third of cases . we present a case of a 47 - year - old hispanic woman who presented with left eye itching and irritation associated with a painless pink mass . physical examination revealed the presence of a ‘ pink salmon - patch ’ involving her left medial conjunctiva . orbital ct showed a subcentimeter left preseptal soft tissue density . biopsy revealed a dense subepithelial lymphoid infiltrate comprised predominantly of b cells that did not coexpress cd5 or cd43 . these findings were consistent with b - cell marginal zone lymphoma . further staging assessment did not reveal disseminated disease . she had stage 1e extranodal marginal zone lymphoma as per ann arbor staging system . she received external beam radiotherapy to her left eye with complete resolution of the lymphoma in 2 months and continues to remain tumour free at 8 - month follow - up . she will be followed up closely for development of any local ( unilateral or contralateral eye ) or systemic recurrence in the long run .  background  this paper reports about ocular marginal zone lymphoma . although this entity is uncommon , its presenting features are similar to the more commonly seen conditions . hence , its early diagnosis may be missed . since this tumour has very good response to radiation , especially when it is localised , early recognition and prompt treatment is imperative . clinicians should be aware of this condition and request a conjunctival biopsy when in doubt . also , since there remains a high risk of progression to systemic lymphoma in the long run , these patients should be monitored closely .  case presentation  a 47 - year - old hispanic woman was seen in the ophthalmology clinic for a 3 - week history of a painless pink mass on the inner aspect of her left eye associated with itching and irritation .  there was no headache , painful eye movements , blurry vision or double vision . she did not have watering of the eye , redness or photophobia . there was no history of use of topical ophthalmic medications . she had no systemic complaints . she had no history of sexually transmitted diseases . her medical history was significant for poorly controlled diabetes mellitus and migraine . she denied smoking , alcohol or illicit drug abuse . her family history was significant for the presence of diabetes mellitus in her mother . on examination , her visual acuity was 20 / 20 in each eye . pupils were normal in size and bilaterally equal reacting to light . ocular motility and intraocular pressures were normal . physical examination of the left eye showed an irregular ‘ pink salmon - patch ’ involving the superomedial aspect of the conjunctiva with extension into the inferior fornix ( figure 1a , b ) . right eye examination was normal . dilated fundus examination showed normal macula , vessels and periphery bilaterally . there were no preauricular , submandibular or cervical lymph nodes palpable . rest of the physical examinations was unremarkable .  investigations  ct of the orbit with contrast showed a subcentimeter left preseptal soft tissue density ( figure 2 ) . brain ct was unremarkable . biopsy of the conjunctival mass showed a dense subepithelial lymphoid infiltrate comprised predominantly of b cells ( figure 3a – c ) that did not coexpress cd5 or cd43 . clonal rearrangement of immunoglobulin heavy chain gene was detected by pcr . these findings were consistent with a b - cell type , marginal zone lymphoma . results of laboratory tests were normal including a complete blood count and serum lactate dehydrogenase ( ldh ) level . chest , abdomen and pelvis cts were unremarkable . 18 - fluorodeoxyglucose - positron emission tomography ( pet ) / ct revealed enhanced metabolism in the left medial conjunctiva . there was no evidence of disseminated disease . bone marrow examination did not reveal any evidence of lymphoma . the patient tested negative for chlamydia psittaci . the patient was thus diagnosed with left conjunctival extranodal marginal zone lymphoma ( emzl ) , stage 1e according to the ann arbor classification .  left inferior conjunctival mass biopsy : dense infiltration of monotonous cells beneath the conjunctival epithelium and even in perivascular areas . cells are arranged in nodular pattern ( h & amp ; e stain × 40 ) . dense infiltration of monotonous cells beneath the conjunctival epithelium ( h & amp ; e stain × 100 ) . small to intermediate - sized lymphocytes with dispersed cytoplasm without nucleoli ( h & amp ; e stain × 400 ) .  differential diagnosis  the key clinical feature is evaluation of a salmon pink mass over the bulbar conjunctiva . differential diagnoses include nodular scleritis , chronic follicular conjunctivitis , fibrovascular pterygium , benign tumours such as squamous papilloma , reactive lymphoid hyperplasia and malignant tumours such as ocular adnexal lymphomas ( oals ) and amelanotic melanoma .  nodular scleritis is associated with systemic diseases in 50 % of cases . these include systemic lupus erythematosus , rheumatoid arthritis , sjogren 's syndrome , wegener 's granulomatosis and spondyloarthropathies . it usually presents with redness and a severe constant boring pain that radiates to the face , worsens at night and with eye movement . this patient had a painless mass which would make this diagnosis less likely . however , autoimmune workup including ana , ds - dna , ra factor , anti-ccp , c - anca , p - anca , ss - a , ss - b and anti-rnp may be sent if in doubt .  pterygium typically starts medially on the nasal conjunctiva and extends laterally onto the cornea in a wedge - shaped fashion that is its distinguishing feature . it is usually soft in consistency , regular in shape and bilateral . this is in contrast to neoplastic lesions that are often more vascular , irregular and unilateral . the mass in this case was irregular and did not involve the cornea making pterygium less likely . treatment involves primary excision of the mass with conjunctival autograft .  chronic follicular conjunctivitis occurs most commonly due to chlamydia trachomatis infection . it is a sexually transmitted disease . clinical features include mucous discharge , crusting of lashes , conjunctival hyperaemia and chemosis . follicular reaction ( cobblestone appearance ) is a key feature and typically involves the bulbar conjunctiva and semilunar folds , which was not seen in this patient . owing to the high prevalence of associated genital tract disease , systemic therapy is preferred . confirmed cases may be treated with doxycycline 100 mg twice daily for 10 days or azithromycin 1 g single dose .  it is thus clear based on the above discussion that initial differential diagnosis of a conjunctival mass is broad and biopsy is essential to rule out malignant aetiologies .  treatment  the patient was treated with localised external beam radiotherapy ( ebrt ) . the total dose given was 3060 cgy with fractions of 180 cgy / day . a tungsten eye shield was used to cover the patient ’ s eye ( except for the area of interest ) during therapy to reduce the risk of radiation keratopathy , cataract and retinopathy .  outcome and follow - up  on follow - up after 2 months , the patient did not report any itching or irritation of her left eye . slit - lamp examination revealed complete resolution of the salmon - patch seen previously . a repeat ct of the orbits demonstrated complete resolution of the left conjunctival lesion ( figure 4 ) . the patient continues to follow - up without any visual complaints and remains tumour free at 8 - month follow - up now . she will have long - term follow - up and will be monitored for development of any local ( unilateral or contralateral eye ) or systemic recurrence . previous studies have shown that the rate of progression to systemic lymphoma is 38 % at 5 years and 79 % at 10 years of the initial diagnosis .  discussion  oals account for 1 – 2 % of all non-hodgkin ' s lymphomas . conjunctiva is the primary site of involvement in 20 – 33 % of cases followed by orbit in 46 – 74 % , eyelid in 5 – 20 % , lacrimal sac in 7.5 % and caruncle in 2.5 % . multiple site and bilateral involvement is seen in 10 – 20 % of patients . ocular adnexa may also be the site of relapse in otherwise systemic lymphomas . emzl of mucosa - associated lymphoid tissue ( malt ) type is the most common subtype , followed by follicular and diffuse large b - cell lymphoma . c. psittaci , helicobacter pylori , borrelia burgdorferi and hepatitis c virus have been linked to malt lymphomas . emzl usually occurs in fifth to seventh decade and has a female predominance .  clinical features are often non-specific . it usually presents as a salmon - pink nodular patch involving the bulbar conjunctiva . features such as rapid growth , invasion of surrounding tissues , ulceration and presence of feeder vessels support a diagnosis of lymphoma . in a review by white et al , 7 common presenting symptoms included mass in 48 % , swelling in 45 % , followed by diplopia , ptosis and proptosis . imaging plays an important role in detecting small , occult or multifocal lesions . ct and / or mri of the brain and orbit help to assess size and location of the tumour . they , however , do not differentiate benign from malignant lesions . they also help in assessing patient 's response to therapy as described in the present case . b - scan ultrasonography is more sensitive in detecting small extrascleral lesions . however , clinical presentation and imaging do not help with the definitive diagnosis . confirmation depends on histopathology , immunophenotype and molecular genetics . immunophenotypic analysis for b - cell and t - cell markers , heavy and light chain restriction , cd5 , cd10 , cd23 , cyclin d1 and bcl - 2 should be performed . flow cytometry helps with assessing quantitative data . mantle cell and marginal cell tumours appear to be histologically similar but can be distinguished based on their differential expression of cd5 . molecular genetic analysis for gene rearrangements of the igg heavy chain can determine clonality . additional staging should follow including laboratory workup ( complete blood count and ldh level ) , chest x-ray , ct of the chest , abdomen and pelvis , and a bone marrow biopsy . a pet scan is more sensitive than ct in determining distant disease .  all patients should be appropriately staged using the conventional ann arbor or the recent american joint committee on cancer tumor - node - metastasis ( tnm - ajcc ) staging system . according to ann arbor staging , patients can be classified as stage i , ii , iii or iv . stage i denotes oal in a localisd area . stage ii indicates lymphoma in two separate areas , an affected lymph node or organ and a second affected area , and both affected areas are confined to above or below the diaphragm . stage iii refers to lymphomas that have spread to involve both sides of the diaphragm ( including 1 organ or area near the lymph nodes or the spleen ) . stage iv indicates disseminated involvement of extralymphatic organs and includes lymphoma with liver , bone marrow or nodular pulmonary disease . modifiers may be used for stage subclassification . ‘ e ’ denotes extranodal disease , ‘ a ’ or ‘ b ’ refers to absence or presence of b symptoms , ‘ s ’ refers to splenic involvement and ‘ x ’ is used if the largest deposit is & gt ; 10 cm , that is , bulky disease . the major pitfall associated with the ann arbor staging is that it does not differentiate among different oals based on anatomic location , bilaterality , multicentricity and extent of infiltration of the primary tumour . hence , it can not determine prognosis in these cases . aronow et al did a retrospective clinical review of 63 patients with primary oal who were staged according to the ajcc - tnm clinical staging system . they determined that the tnm system was indeed useful for precise characterisation of the extent of local disease . although the t stage did not predict relapse or survival , n1 - 4 and m1 stages were able to determine less favourable survival outcomes .  localised conjunctival lymphoma has a good prognosis . it usually has normal serum ldh levels , absence of b symptoms and follows an indolent course . twenty per cent of patients with localised disease will develop disseminated disease . less than 5 % of patients die from the lymphoma . unfavourable prognostic indicators previously identified include advanced age at diagnosis , female , prior history of lymphoma , higher disease stage , elevated international prognostic index score , nodal involvement , non-conjunctival primary site , bilaterality at presentation , age older than 60 years , b symptoms and elevated ldh levels . relapses usually involve the contralateral orbit . high - grade transformation may occur in 1 – 3 % of cases .  treatment depends on whether the disease is localised or disseminated . localised disease may be treated with ebrt , intralesional injections of interferon alpha 2b or rituximab , or local excision . 5 surgical resection may be associated with a high risk of recurrence due to presence of tumour microinvasion in the surrounding tissues . elderly and frail patients or those with significant comorbidities and asymptomatic disease may be offered a ‘ wait - and - watch ’ strategy . ebrt has a high response rate , good local control and a 4 - year relapse rate of 20 – 25 % . russell et al10 showed that a median radiation dose of 30 gy led to excellent local control and was well tolerated with expected cataractogenesis . patients with recurrence were also salvaged successfully . the entire conjunctiva on the involved site should be irradiated . this is because conjunctiva is a lymphoid - rich tissue compared with other structures in the orbit . if it is not completely included in the radiation field , it may lead to local relapses . complications of ebrt include dry eye , cataract , glaucoma , retinal bleeding and retinopathy . radiation - induced retinopathy and retinal bleeding usually occur with radiation doses above 40 gy . disseminated disease is treated with either chemotherapy ( eg , cyclophosphamide , doxorubicin , vincristine and prednisone , ie , chop with or without rituximab ) , immunotherapy ( rituximab ) or radioimmunotherapy ( using 90y - ibritumomab tiuxetan ) . rituximab is a chimeric monoclonal antibody targeted against the cd20 antigen on b lymphocytes . 90yttrium - ibritumomab tiuxetan as a modality of targeted radiotherapy offers the advantage of delivering a lower radiation dose than ebrt and at the same time targeting malignant cells throughout the body . it can be used for rituximab refractory disease . this is because the two of them have different mechanisms of action . while rituximab utilises host effector mechanisms to kill tumour cells , 90yttrium - ibritumomab tiuxetan acts directly through emission of beta particles . the eradication of c. psittaci infection with doxycycline ( 100 mg administered orally twice a day , for 3 weeks ) has been proposed as an effective strategy but remains controversial . 6\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_processor.paragraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train and dev data loaders\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_data_processor, shuffle=True, batch_size=bs, collate_fn=test_data_processor.my_collate)\n",
    "print('Created train and dev data loaders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  7 11:39:27 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:18:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    58W / 300W |   2962MiB / 32510MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0    240102      C   /packages/7x/anaconda3/5.3.0/bin/python     2951MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating testing on test set\n",
      "Test loss: 0.057050696317094585\n",
      "Test Accuracy: 0.9731057468293697\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      ans       0.40      0.29      0.34     67677\n",
      "    [PAD]       0.00      0.00      0.00     69475\n",
      "\n",
      "micro avg       0.40      0.14      0.21    137152\n",
      "macro avg       0.20      0.14      0.17    137152\n",
      "\n",
      "F1-Score: 0.21097123265655973\n"
     ]
    }
   ],
   "source": [
    "print('Initiating testing on test set')\n",
    "# Testing on test set\n",
    "model.eval()\n",
    "mynet.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "predictions , true_labels = [], []\n",
    "for batch in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        # add batch to gpu\n",
    "        for split in range(len(batch[0])):\n",
    "            b_input_ids = batch[0].pop(0).to(device)\n",
    "            b_input_mask = batch[1].pop(0).to(device)\n",
    "            b_labels = batch[2].pop(0).to(device)\n",
    "            b_ttids = batch[3].pop(0).to(device)\n",
    "           # forward pass\n",
    "            # loss = model(b_input_ids, token_type_ids=b_ttids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            if split == 0:\n",
    "                last_hidden_state = model(b_input_ids, token_type_ids=b_ttids, attention_mask=b_input_mask)[0]\n",
    "            else:\n",
    "                last_hidden_state = torch.cat([last_hidden_state,model(b_input_ids, token_type_ids=b_ttids, attention_mask=b_input_mask)[0]], dim = 1)\n",
    "                b_labels = torch.cat([prev_labels,b_labels],dim=1)\n",
    "            del b_input_ids, b_input_mask, b_ttids\n",
    "            torch.cuda.empty_cache()\n",
    "            prev_labels = copy.copy(b_labels)\n",
    "        del prev_labels\n",
    "        torch.cuda.empty_cache()\n",
    "        b_logits = mynet(last_hidden_state)\n",
    "        b_logits = b_logits.transpose(1,2)\n",
    "        logits = b_logits.view(-1,len(tags_vals))\n",
    "        labels = b_labels.view(-1)\n",
    "        tmp_eval_loss = closs(logits, labels)\n",
    "        del logits, labels\n",
    "        torch.cuda.empty_cache()\n",
    "    b_logits = b_logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    predictions.extend([list(p) for p in np.argmax(b_logits, axis=2)])\n",
    "    true_labels.append(label_ids)\n",
    "    tmp_eval_accuracy = flat_accuracy(b_logits, label_ids)\n",
    "    del b_logits, label_ids, b_labels\n",
    "    torch.cuda.empty_cache()\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_examples += 1\n",
    "    nb_eval_steps += 1\n",
    "eval_loss = eval_loss/nb_eval_steps\n",
    "print(\"Test loss: {}\".format(eval_loss))\n",
    "print(\"Test Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "valid_tags = [tags_vals[l_ii] if l_ii!=-100 else '[PAD]' for l in true_labels for l_i in l for l_ii in l_i]\n",
    "c_f1 = f1_score(valid_tags, pred_tags)\n",
    "rep = classification_report(valid_tags, pred_tags)\n",
    "print(rep)\n",
    "print(\"F1-Score: {}\".format(c_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_NER_agave.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Py3-basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
