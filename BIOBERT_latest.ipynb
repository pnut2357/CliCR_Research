{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "colab_type": "code",
    "id": "XNEJmEyguUt-",
    "outputId": "c72594cd-c019-478d-8bf7-4074ec470c8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'! pip install pytorch_pretrained_bert\\n#! pip install pytorch_transformers\\n! pip install seqeval\\n! pip install transformers'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"! pip install pytorch_pretrained_bert\n",
    "#! pip install pytorch_transformers\n",
    "! pip install seqeval\n",
    "! pip install transformers\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1bX2m-OitOPV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0505 14:50:35.425671 47764350328512 file_utils.py:41] PyTorch version 1.2.0 available.\n",
      "I0505 14:50:42.227373 47764350328512 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertModel,BertTokenizer, BertForTokenClassification\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "vtIuVG38Qi2d",
    "outputId": "b8edca0b-275c-428c-8757-9eb924137f3a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "59J1IoQbuZOI"
   },
   "outputs": [],
   "source": [
    "# from pytorch_pretrained_bert import WEIGHTS_NAME, CONFIG_NAME, BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Gdd-0Xluuehu",
    "outputId": "132a4552-abcb-4688-f065-de29b88a0dad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from google.colab import drive\\ndrive.mount(\"/content/drive/\")'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from google.colab import drive\n",
    "drive.mount(\"/content/drive/\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "0qn6c5OIulzQ",
    "outputId": "152d6291-d121-4451-f95d-72e1802540ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%cd drive/My\\\\ Drive/data\\n!ls'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"%cd drive/My\\ Drive/data\n",
    "!ls\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FBqOlEv0Nt0t"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0505 14:50:43.679851 47764350328512 tokenization_utils.py:420] Model name './biobert_v1.1_pubmed/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming './biobert_v1.1_pubmed/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0505 14:50:43.682863 47764350328512 tokenization_utils.py:449] Didn't find file ./biobert_v1.1_pubmed/added_tokens.json. We won't load it.\n",
      "I0505 14:50:43.683643 47764350328512 tokenization_utils.py:449] Didn't find file ./biobert_v1.1_pubmed/special_tokens_map.json. We won't load it.\n",
      "I0505 14:50:43.684222 47764350328512 tokenization_utils.py:449] Didn't find file ./biobert_v1.1_pubmed/tokenizer_config.json. We won't load it.\n",
      "I0505 14:50:43.684889 47764350328512 tokenization_utils.py:502] loading file ./biobert_v1.1_pubmed/vocab.txt\n",
      "I0505 14:50:43.685410 47764350328512 tokenization_utils.py:502] loading file None\n",
      "I0505 14:50:43.686041 47764350328512 tokenization_utils.py:502] loading file None\n",
      "I0505 14:50:43.686561 47764350328512 tokenization_utils.py:502] loading file None\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./biobert_v1.1_pubmed/\"\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir, do_lower_case=True)  # Add specific options if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q-2upMVDuqJW"
   },
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "colab_type": "code",
    "id": "iYipQwBPtOPb",
    "outputId": "5a60de56-a728-4835-e5d1-17fe5d40a886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May  5 14:50:44 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:18:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    58W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "torch.cuda.get_device_name(0) \n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFM4ddFtwmJm"
   },
   "source": [
    "#### Load Data from Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QsLc7eMHtOPf"
   },
   "outputs": [],
   "source": [
    "#from my_process_json import *\n",
    "from create_processed_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rEMPp7yNM0dA"
   },
   "outputs": [],
   "source": [
    "train_data_processor = DataProcessor('train',size=None,tokenizer=tokenizer)\n",
    "valid_data_processor =  DataProcessor('dev',size=None,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9riB1lggNBe5"
   },
   "outputs": [],
   "source": [
    "tags_vals = train_data_processor.get_tags_vals()\n",
    "tr_inputs,tr_masks,tr_tags,tr_ttids,_ = train_data_processor.get_processed_data()\n",
    "\n",
    "val_inputs,val_masks,val_tags,val_ttids, _ = valid_data_processor.get_processed_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "RLj0HmcKnnGR",
    "outputId": "aa9739c6-9dd2-4805-e207-539258afe78f"
   },
   "outputs": [],
   "source": [
    "train_data_processor.dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "V229iO2fWJ4B",
    "outputId": "fce11803-86d9-4961-8842-bc1c777b83e4"
   },
   "outputs": [],
   "source": [
    "val_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "684jKh3XtOQu"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "tr_ttids = torch.tensor(tr_ttids)\n",
    "\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "val_ttids = torch.tensor(val_ttids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uqUiboWTtOQw"
   },
   "outputs": [],
   "source": [
    "bs = 16\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags , tr_ttids)\n",
    "train_sampler = SequentialSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fcp3Sox-Ukk6"
   },
   "outputs": [],
   "source": [
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags, val_ttids)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "s9bmgMLwtOQz",
    "outputId": "f9047149-69ed-4764-b52e-dd8bf110771d"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k1EaNrfntOQ1"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Re-load the saved model and vocabulary\n",
    "\n",
    "# Example for a Bert model\n",
    "model = BertForTokenClassification.from_pretrained(output_dir, num_labels=len(tags_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pwkdFKpbtOQ5"
   },
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "AOeTWaXHtOQ7",
    "outputId": "7b9cf9ab-c594-493a-f3ce-54ac09e9f948"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oAzaPcYrtOQ-"
   },
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bmFhtXYTvlwo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3A7VD7tgtORC"
   },
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "-uEPX0rP_rhe",
    "outputId": "b400115a-1173-4f49-9dbe-d4284ff38906"
   },
   "outputs": [],
   "source": [
    "print('Initiating training on train set')\n",
    "closs = CrossEntropyLoss(ignore_index=-100)\n",
    "epochs = 4\n",
    "max_grad_norm = 1.0\n",
    "F_scores = np.zeros(epochs,float)\n",
    "patience = 6\n",
    "for e in trange(epochs):\n",
    "    # TRAIN loopmodel.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels, b_ttids = batch\n",
    "\n",
    "        logits = model(b_input_ids, token_type_ids=b_ttids,attention_mask=b_input_mask)[0]\n",
    "        blogits = logits.view(-1, len(tags_vals))\n",
    "        labels = b_labels.view(-1)\n",
    "        loss = closs(blogits,labels)\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_vals[l_ii] if l_ii!=-100 else '[PAD]' for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    c_f1 = f1_score(valid_tags, pred_tags)\n",
    "    rep = classification_report(valid_tags, pred_tags)\n",
    "    print('Training classification report:',rep)\n",
    "    print(\"Training F1-Score: {}\".format(c_f1))\n",
    "\n",
    "\n",
    "    \n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels, b_ttids = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #tmp_eval_loss = model(b_input_ids, token_type_ids=b_ttids,attention_mask=b_input_mask, labels=b_labels)\n",
    "            logits = model(b_input_ids, token_type_ids=b_ttids,attention_mask=b_input_mask)[0]\n",
    "            closs = CrossEntropyLoss(ignore_index=-100)\n",
    "            blogits = logits.view(-1, len(tags_vals))\n",
    "            labels = b_labels.view(-1)\n",
    "            tmp_eval_loss = closs(blogits,labels)\n",
    "\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        true_labels.append(label_ids)\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n",
    "    valid_tags = [[tags_vals[l_ii] if l_ii!=-100 else '[PAD]' for l_ii in l_i] for l in true_labels for l_i in l]\n",
    "    print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"Validation F1-Score: {}\".format(f1_score(valid_tags, pred_tags)))\n",
    "    rep = classification_report(valid_tags, pred_tags)\n",
    "    print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "bGdnsrBptORF",
    "outputId": "dfa76535-f5e1-4d17-adb6-e7bb62cab73a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating validation on dev set\n",
      "Validation loss: 0.05722865881327313\n",
      "Validation Accuracy: 0.571827906976744\n",
      "Validation F1-Score: 0.18935939602332186\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "      ans       0.41      0.32      0.36     63389\n",
      "    [PAD]       0.00      0.00      0.00    101103\n",
      "\n",
      "micro avg       0.41      0.12      0.19    164492\n",
      "macro avg       0.16      0.12      0.14    164492\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Initiating validation on dev set')\n",
    "# VALIDATION on validation set\n",
    "model.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "predictions , true_labels = [], []\n",
    "for batch in valid_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels, b_ttids = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #tmp_eval_loss = model(b_input_ids, token_type_ids=b_ttids,attention_mask=b_input_mask, labels=b_labels)\n",
    "        logits = model(b_input_ids, token_type_ids=b_ttids,attention_mask=b_input_mask)[0]\n",
    "        closs = CrossEntropyLoss(ignore_index=-100)\n",
    "        blogits = logits.view(-1, len(tags_vals))\n",
    "        labels = b_labels.view(-1)\n",
    "        tmp_eval_loss = closs(blogits,labels)\n",
    "\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    true_labels.append(label_ids)\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n",
    "valid_tags = [[tags_vals[l_ii] if l_ii!=-100 else '[PAD]' for l_ii in l_i] for l in true_labels for l_i in l]\n",
    "print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Validation F1-Score: {}\".format(f1_score(valid_tags, pred_tags)))\n",
    "rep = classification_report(valid_tags, pred_tags)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6TIWXLdvV_Qp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VKM4XraSV_Xv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0507 10:23:04.902904 47764350328512 modeling.py:230] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8bdac2a6d6a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_save\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_model_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel_to_save\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtrain_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "print('Saving model')\n",
    "from pytorch_pretrained_bert import WEIGHTS_NAME, CONFIG_NAME\n",
    "\n",
    "output_dir = \"./model_notsure/\"\n",
    "\n",
    "# Step 1: Save a model, configuration and vocabulary that you have fine-tuned\n",
    "\n",
    "# If we have a distributed model, save only the encapsulated model\n",
    "# (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "\n",
    "# If we save using the predefined names, we can load using `from_pretrained`\n",
    "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "tokenizer.save_vocabulary(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oI_6kQ5_V_Vd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model_notsure/vocab.txt',)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2wfusz0V_Oc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4u4Xxb0V_MJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data\n"
     ]
    }
   ],
   "source": [
    "print('Loading test data')\n",
    "test_data_processor = DataProcessor('test',size=None,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs,test_masks,test_tags,test_ttids, _ = test_data_processor.get_processed_data()\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "test_masks = torch.tensor(test_masks)\n",
    "test_tags = torch.tensor(test_tags)\n",
    "test_ttids = torch.tensor(test_ttids)\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_tags, test_ttids)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xQ7AsMNPV_Gt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  7 12:12:02 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:18:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    59W / 300W |  15676MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     35424      C   /packages/7x/anaconda3/5.3.0/bin/python    15665MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating validation on dev set\n"
     ]
    }
   ],
   "source": [
    "print('Initiating testing on test set')\n",
    "# VALIDATION on validation set\n",
    "model.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "predictions , true_labels = [], []\n",
    "for batch in test_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels, b_ttids = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #tmp_eval_loss = model(b_input_ids, token_type_ids=b_ttids,attention_mask=b_input_mask, labels=b_labels)\n",
    "        logits = model(b_input_ids, token_type_ids=b_ttids,attention_mask=b_input_mask)[0]\n",
    "        closs = CrossEntropyLoss(ignore_index=-100)\n",
    "        blogits = logits.view(-1, len(tags_vals))\n",
    "        labels = b_labels.view(-1)\n",
    "        tmp_eval_loss = closs(blogits,labels)\n",
    "\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    true_labels.append(label_ids)\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n",
    "valid_tags = [[tags_vals[l_ii] if l_ii!=-100 else '[PAD]' for l_ii in l_i] for l in true_labels for l_i in l]\n",
    "print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Validation F1-Score: {}\".format(f1_score(valid_tags, pred_tags)))\n",
    "rep = classification_report(valid_tags, pred_tags)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "BIOBERT_latest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Py3-basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
